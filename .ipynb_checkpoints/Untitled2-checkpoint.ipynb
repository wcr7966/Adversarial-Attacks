{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>christma tree farm pictur</td>\n",
       "      <td>ham</td>\n",
       "      <td>spampy/datasets/enron\\enron1\\ham\\0001.1999-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vastar resourc inc gari product high island la...</td>\n",
       "      <td>ham</td>\n",
       "      <td>spampy/datasets/enron\\enron1\\ham\\0002.1999-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calpin daili ga nomin calpin daili ga nomin doc</td>\n",
       "      <td>ham</td>\n",
       "      <td>spampy/datasets/enron\\enron1\\ham\\0003.1999-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>issu fyi see note alreadi done stella forward ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>spampy/datasets/enron\\enron1\\ham\\0004.1999-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meter nov alloc fyi forward lauri allen hou ec...</td>\n",
       "      <td>ham</td>\n",
       "      <td>spampy/datasets/enron\\enron1\\ham\\0005.1999-12-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33711</th>\n",
       "      <td>iso q good news c edaliss val edumm vl eoggra ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spampy/datasets/enron\\enron6\\spam\\5995.2005-07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33712</th>\n",
       "      <td>prescript medicin special precis put buck back...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spampy/datasets/enron\\enron6\\spam\\5997.2005-07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33713</th>\n",
       "      <td>next gener onlin pharmaci readi rock let man r...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spampy/datasets/enron\\enron6\\spam\\5998.2005-07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33714</th>\n",
       "      <td>bloow time time learn last time longer bed rea...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spampy/datasets/enron\\enron6\\spam\\5999.2005-07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33715</th>\n",
       "      <td>dear sir interest hi need softwar give link ht...</td>\n",
       "      <td>spam</td>\n",
       "      <td>spampy/datasets/enron\\enron6\\spam\\6000.2005-07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33716 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message label  \\\n",
       "0                             christma tree farm pictur    ham   \n",
       "1      vastar resourc inc gari product high island la...   ham   \n",
       "2       calpin daili ga nomin calpin daili ga nomin doc    ham   \n",
       "3      issu fyi see note alreadi done stella forward ...   ham   \n",
       "4      meter nov alloc fyi forward lauri allen hou ec...   ham   \n",
       "...                                                  ...   ...   \n",
       "33711  iso q good news c edaliss val edumm vl eoggra ...  spam   \n",
       "33712  prescript medicin special precis put buck back...  spam   \n",
       "33713  next gener onlin pharmaci readi rock let man r...  spam   \n",
       "33714  bloow time time learn last time longer bed rea...  spam   \n",
       "33715  dear sir interest hi need softwar give link ht...  spam   \n",
       "\n",
       "                                                    path  \n",
       "0      spampy/datasets/enron\\enron1\\ham\\0001.1999-12-...  \n",
       "1      spampy/datasets/enron\\enron1\\ham\\0002.1999-12-...  \n",
       "2      spampy/datasets/enron\\enron1\\ham\\0003.1999-12-...  \n",
       "3      spampy/datasets/enron\\enron1\\ham\\0004.1999-12-...  \n",
       "4      spampy/datasets/enron\\enron1\\ham\\0005.1999-12-...  \n",
       "...                                                  ...  \n",
       "33711  spampy/datasets/enron\\enron6\\spam\\5995.2005-07...  \n",
       "33712  spampy/datasets/enron\\enron6\\spam\\5997.2005-07...  \n",
       "33713  spampy/datasets/enron\\enron6\\spam\\5998.2005-07...  \n",
       "33714  spampy/datasets/enron\\enron6\\spam\\5999.2005-07...  \n",
       "33715  spampy/datasets/enron\\enron6\\spam\\6000.2005-07...  \n",
       "\n",
       "[33716 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "csv.field_size_limit(500 * 1024 * 1024)\n",
    "with open(\"messages.csv\", \"r\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    messages = []\n",
    "    for row in reader:\n",
    "        messages.append(row[1:])\n",
    "messages = messages[1:]\n",
    "messages = pd.DataFrame(messages, columns=['message', 'label', 'path'])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test and store the path in x_train_path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "messages['label'] = messages['label'].replace('ham', 0)\n",
    "messages['label'] = messages['label'].replace('spam', 1)\n",
    "\n",
    "messages_label = messages['label']\n",
    "message_path = messages['path']\n",
    "x = messages['message']\n",
    "y = messages_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>fw special issuealert big texa news wood ferc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30513</th>\n",
       "      <td>pre qualifi appli home loan mortgag novemb upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33368</th>\n",
       "      <td>new softwar breacher annul spot young want pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24089</th>\n",
       "      <td>hotel show last chanc exhibit hotel show may a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29485</th>\n",
       "      <td>percent med levitra cial save xanax valium phe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12644</th>\n",
       "      <td>global market monitor prepar presentaton argen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7655</th>\n",
       "      <td>confirm order automat confirm order place use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17685</th>\n",
       "      <td>start date hourahead hour start date hourahead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>softwar licens karla august vacat month franc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25513</th>\n",
       "      <td>readi get hello viagra med struggl men erectil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6743 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message\n",
       "12481  fw special issuealert big texa news wood ferc ...\n",
       "30513  pre qualifi appli home loan mortgag novemb upd...\n",
       "33368  new softwar breacher annul spot young want pow...\n",
       "24089  hotel show last chanc exhibit hotel show may a...\n",
       "29485  percent med levitra cial save xanax valium phe...\n",
       "...                                                  ...\n",
       "12644  global market monitor prepar presentaton argen...\n",
       "7655   confirm order automat confirm order place use ...\n",
       "17685  start date hourahead hour start date hourahead...\n",
       "6726   softwar licens karla august vacat month franc ...\n",
       "25513  readi get hello viagra med struggl men erectil...\n",
       "\n",
       "[6743 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p41, p5, p41_y, p5_y = train_test_split(x, y, test_size=0.2)\n",
    "p31, p4, p31_y, p4_y = train_test_split(p41, p41_y, test_size=0.25)\n",
    "p21, p3, p21_y, p3_y = train_test_split(p31, p31_y, test_size=1/3)\n",
    "p1, p2, p1_y, p2_y = train_test_split(p21, p21_y, test_size=0.5)\n",
    "\n",
    "pd_p1 = pd.DataFrame(p1)\n",
    "pd_p1.to_csv(\"pd_p1.csv\")\n",
    "pd_p2 = pd.DataFrame(p2)\n",
    "pd_p2.to_csv(\"pd_p2.csv\")\n",
    "pd_p3 = pd.DataFrame(p3)\n",
    "pd_p3.to_csv(\"pd_p3.csv\")\n",
    "pd_p4 = pd.DataFrame(p4)\n",
    "pd_p4.to_csv(\"pd_p4.csv\")\n",
    "pd_p5 = pd.DataFrame(p5)\n",
    "pd_p5.to_csv(\"pd_p5.csv\")\n",
    "\n",
    "pd_p1_y = pd.DataFrame(p1_y)\n",
    "pd_p1_y.to_csv(\"pd_p1_y.csv\")\n",
    "pd_p2_y = pd.DataFrame(p2_y)\n",
    "pd_p2_y.to_csv(\"pd_p2_y.csv\")\n",
    "pd_p3_y = pd.DataFrame(p3_y)\n",
    "pd_p3_y.to_csv(\"pd_p3_y.csv\")\n",
    "pd_p4_y = pd.DataFrame(p4_y)\n",
    "pd_p4_y.to_csv(\"pd_p4_y.csv\")\n",
    "pd_p5_y = pd.DataFrame(p5_y)\n",
    "pd_p5_y.to_csv(\"pd_p5_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myopen(mydir):\n",
    "    pd1 = []\n",
    "    index1 = []\n",
    "    with open(mydir, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for j, row in reader:\n",
    "            index1.append(j)\n",
    "            if row == '0' or row == '1':\n",
    "                pd1.append(int(row))\n",
    "            else: \n",
    "                pd1.append(row)\n",
    "    pd1 = pd1[1:]\n",
    "    index1 = index1[1:]\n",
    "    pd1 = pd.Series(pd1, index = index1)\n",
    "    return pd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_p1 = myopen(\"pd_p1.csv\")\n",
    "pd_p2 = myopen(\"pd_p2.csv\")\n",
    "pd_p3 = myopen(\"pd_p3.csv\")\n",
    "pd_p4 = myopen(\"pd_p4.csv\")\n",
    "pd_p5 = myopen(\"pd_p5.csv\")\n",
    "\n",
    "pd_p1_y = myopen(\"pd_p1_y.csv\")\n",
    "pd_p2_y = myopen(\"pd_p2_y.csv\")\n",
    "pd_p3_y = myopen(\"pd_p3_y.csv\")\n",
    "pd_p4_y = myopen(\"pd_p4_y.csv\")\n",
    "pd_p5_y = myopen(\"pd_p5_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd_p1.append(pd_p2).append(pd_p3).append(pd_p5)\n",
    "x_test = pd_p4\n",
    "\n",
    "y_train = pd_p1_y.append(pd_p2_y).append(pd_p3_y).append(pd_p5_y)\n",
    "y_test = pd_p4_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6743\n",
      "6743\n"
     ]
    }
   ],
   "source": [
    "print(len(pd_p1))\n",
    "print(len(pd_p1_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path = []\n",
    "for i, v in x_train.items():\n",
    "    i = int(i)\n",
    "    x_train_path.append((i, message_path[i]))\n",
    "    \n",
    "x_test_path = []\n",
    "for i, v in x_test.items():\n",
    "    i = int(i)\n",
    "    x_test_path.append((i, message_path[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (26973, 106292) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-32ba0451a7f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtfidf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtfidf_matrix_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtfidf_matrix_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate array with shape (26973, 106292) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Tf-idf for train datasets\n",
    "vect = TfidfVectorizer()\n",
    "tfidf_train = vect.fit_transform(x_train)\n",
    "tfidf_matrix_train = pd.DataFrame(tfidf_train.toarray(), columns = vect.get_feature_names())\n",
    "headers = vect.get_feature_names()\n",
    "tfidf_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaci</th>\n",
       "      <th>aaaahhhhhh</th>\n",
       "      <th>aaadrizzl</th>\n",
       "      <th>aaaenerfax</th>\n",
       "      <th>aaagrp</th>\n",
       "      <th>aaal</th>\n",
       "      <th>aaasash</th>\n",
       "      <th>...</th>\n",
       "      <th>zztc</th>\n",
       "      <th>zzucpkow</th>\n",
       "      <th>zzw</th>\n",
       "      <th>zzxtfeerekvwkug</th>\n",
       "      <th>zzxxst</th>\n",
       "      <th>zzyudgpd</th>\n",
       "      <th>zzzglvaa</th>\n",
       "      <th>zzzxlqbha</th>\n",
       "      <th>zzzz</th>\n",
       "      <th>zzzzcard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>0.133343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6740</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6741</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6742</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6743 rows × 107866 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            aa  aaa  aaaa  aaaaci  aaaahhhhhh  aaadrizzl  aaaenerfax  aaagrp  \\\n",
       "0     0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "1     0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "2     0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "3     0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "4     0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "...        ...  ...   ...     ...         ...        ...         ...     ...   \n",
       "6738  0.133343  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "6739  0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "6740  0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "6741  0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "6742  0.000000  0.0   0.0     0.0         0.0        0.0         0.0     0.0   \n",
       "\n",
       "      aaal  aaasash  ...  zztc  zzucpkow  zzw  zzxtfeerekvwkug  zzxxst  \\\n",
       "0      0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "1      0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "2      0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "3      0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "4      0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "...    ...      ...  ...   ...       ...  ...              ...     ...   \n",
       "6738   0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "6739   0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "6740   0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "6741   0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "6742   0.0      0.0  ...   0.0       0.0  0.0              0.0     0.0   \n",
       "\n",
       "      zzyudgpd  zzzglvaa  zzzxlqbha  zzzz  zzzzcard  \n",
       "0          0.0       0.0        0.0   0.0       0.0  \n",
       "1          0.0       0.0        0.0   0.0       0.0  \n",
       "2          0.0       0.0        0.0   0.0       0.0  \n",
       "3          0.0       0.0        0.0   0.0       0.0  \n",
       "4          0.0       0.0        0.0   0.0       0.0  \n",
       "...        ...       ...        ...   ...       ...  \n",
       "6738       0.0       0.0        0.0   0.0       0.0  \n",
       "6739       0.0       0.0        0.0   0.0       0.0  \n",
       "6740       0.0       0.0        0.0   0.0       0.0  \n",
       "6741       0.0       0.0        0.0   0.0       0.0  \n",
       "6742       0.0       0.0        0.0   0.0       0.0  \n",
       "\n",
       "[6743 rows x 107866 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf-idf for test datsets\n",
    "tfidf_test = vect.transform(x_test)\n",
    "tfidf_matrix_test = pd.DataFrame(tfidf_test.toarray(), columns = vect.get_feature_names())\n",
    "tfidf_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build SVM\n",
      "Find the best params\n",
      "Finish Train\n",
      "The best training parameters are:  [('C', 1)]\n",
      "Train SVM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CClassifierSVM{'classes': CArray(2,)(dense: [0 1]), 'n_features': 107866, 'preprocess': None, 'n_jobs': 1, 'C': 1.0, 'class_weight': None, 'w': CArray(1, 107866)(sparse: (0, 83145) 0.04679116481735237  (0, 72448) 0.04061899279297929  (0, 22769) 0.029692461001204856  (0, 106114) 0.049273440526520794  (0, 92723) 0.052902621367772434  (0, 92255) 0.052902621367772434  (0, 77502) 0.0550255560682403  (0, 74920) 0.0550255560682403  (0, 74672) 0.052902621367772434  (0, 54098) 0.052902621367772434  (0, 36967) 0.052902621367772434  (0, 29196) 0.0550255560682403  (0, 27157) 0.05139637522698866  (0, 26996) 0.0550255560682403  (0, 23512) 0.04715050582605294  (0, 20027) 0.052902621367772434  (0, 6575) 0.05139637522698866  (0, 102504) 0.013082098645286654  (0, 102141) 0.014005846702560302  (0, 86139) 0.014005846702560302  (0, 83884) 0.013082098645286654  (0, 74149) 0.014005846702560302  (0, 69061) 0.011733951522846715  (0, 56830) 0.012158350588013008  (0, 54370) 0.012784718193052647  : :  (0, 32989) -0.40011295217428017  (0, 32841) -0.10128395833984655  (0, 31520) -0.3154621960748  (0, 30793) -0.10602800644766541  (0, 22090) 0.26390288091437875  (0, 21610) -0.05592764023463016  (0, 18424) -2.0196657055222627  (0, 18421) -0.6798173008833386  (0, 17646) -0.2115412601619548  (0, 15121) -1.4147904400308564  (0, 14814) -0.38666736119329825  (0, 13948) -0.34929064836053036  (0, 13803) -0.2248152142871458  (0, 13722) 0.12986417629116442  (0, 9557) -0.25326851235017456  (0, 9433) 0.04626467085413244  (0, 8979) 0.6877813097103211  (0, 6935) -0.17970916508381757  (0, 5737) -0.6227579783521625  (0, 5727) -0.8320466981739565  (0, 4600) -0.4641417766284378  (0, 2960) -0.04855378871287731  (0, 2868) -0.4126304776854762  (0, 940) -0.9589296624774625  (0, 0) 0.10832294421969908), 'b': CArray(1,)(dense: [0.537087]), 'alpha': None, 'sv_idx': None, 'kernel': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from secml.data import CDataset\n",
    "from secml.data.splitter import CDataSplitterKFold\n",
    "from secml.ml.classifiers import CClassifierSVM\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "from secml.ml.peval.metrics import CMetricConfusionMatrix\n",
    "\n",
    "from secml.ml.classifiers.multiclass import CClassifierMulticlassOVA\n",
    "from secml.ml.kernels import CKernelLinear\n",
    "\n",
    "nb_col = tfidf_train.size\n",
    "\n",
    "tr_set = CDataset(tfidf_train, y_train)\n",
    "\n",
    "\n",
    "# Train the SVM\n",
    "print(\"Build SVM\")\n",
    "xval_splitter = CDataSplitterKFold()\n",
    "clf_lin = CClassifierSVM()\n",
    "\n",
    "\n",
    "xval_lin_params = {'C': [1]}\n",
    "\n",
    "print(\"Find the best params\")\n",
    "\n",
    "best_lin_params = clf_lin.estimate_parameters(\n",
    "    dataset = tr_set,\n",
    "    parameters = xval_lin_params,\n",
    "    splitter = xval_splitter,\n",
    "    metric = 'accuracy',\n",
    "    perf_evaluator = 'xval'\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Finish Train\")\n",
    "print(\"The best training parameters are: \", [(k, best_lin_params[k]) for k in sorted(best_lin_params)])\n",
    "\n",
    "print(\"Train SVM\")\n",
    "clf_lin.fit(tr_set.X, tr_set.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDataset{'X': CArray(6743, 107866)(sparse: (0, 104652) 0.3092936352252673  (0, 86791) 0.14511381792644437  (0, 83451) 0.0899420371362185  (0, 80820) 0.20052549704286135  (0, 78494) 0.22365191809163562  (0, 77593) 0.12381612477946713  (0, 77485) 0.32168073742327913  (0, 64473) 0.3064596550829263  (0, 63875) 0.1526803904114166  (0, 56292) 0.22579083209439044  (0, 34130) 0.08808319774788687  (0, 32706) 0.10503535032656006  (0, 30489) 0.137097811949855  (0, 28914) 0.23702652882749636  (0, 27114) 0.24499882064094905  (0, 19404) 0.10017563439119699  (0, 19190) 0.22110562765448663  (0, 17593) 0.07158938192019255  (0, 15791) 0.3931272730589497  (0, 12523) 0.09218818151169217  (0, 6014) 0.0976228087689345  (0, 3229) 0.32790432434713673  (1, 105505) 0.03925891467502661  (1, 104025) 0.04039018708927006  (1, 103513) 0.0331023525296702  : :  (6742, 19404) 0.07484370126329548  (6742, 19220) 0.03691252580019675  (6742, 18570) 0.03184233232771374  (6742, 18351) 0.030663452200802188  (6742, 18251) 0.03939895580836187  (6742, 17750) 0.03306229878910982  (6742, 15817) 0.036646263064774  (6742, 13948) 0.06599531292275915  (6742, 13774) 0.04587784841936774  (6742, 13173) 0.04709189771692897  (6742, 12562) 0.058087907535389616  (6742, 12270) 0.04534335662229735  (6742, 9601) 0.049062370763246384  (6742, 8979) 0.023632304851948987  (6742, 8525) 0.028832627978162644  (6742, 7703) 0.10541637010893143  (6742, 7076) 0.11733997315867081  (6742, 6025) 0.03883771721366914  (6742, 4507) 0.04569561290090929  (6742, 4470) 0.04202223955754778  (6742, 2883) 0.02221230017279367  (6742, 2868) 0.03135167176152246  (6742, 2627) 0.05726040620527413  (6742, 1967) 0.04446435347824172  (6742, 940) 0.027345450820324788), 'Y': CArray(6743,)(dense: [0 1 1 ... 0 0 0]), 'header': None}\n",
      "Accuracy on test set: 99.10%\n",
      "Confusion Matrix: \n",
      "CArray([[3238   51]\n",
      " [  10 3444]])\n",
      "False Positive Rate: 1.12%\n",
      "False Negative Rate: 0.47%\n"
     ]
    }
   ],
   "source": [
    "# Test the Classifier\n",
    "ts_set = CDataset(tfidf_test, y_test)\n",
    "print(ts_set)\n",
    "\n",
    "y_pred = clf_lin.predict(ts_set.X)\n",
    "metric = CMetricAccuracy()\n",
    "acc = metric.performance_score(y_true=ts_set.Y, y_pred=y_pred)\n",
    "\n",
    "confusion_matrix = CMetricConfusionMatrix() \n",
    "cm = confusion_matrix.performance_score(y_true=ts_set.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(cm)\n",
    "print(\"False Positive Rate: {:.2%}\".format(39/(39+3445)))\n",
    "\n",
    "print(\"False Negative Rate: {:.2%}\".format(15/(15+3203)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25554,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\2109.2005-06-25.SA_and_HP.spam.txt'),\n",
       " (26042,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\2809.2005-06-30.SA_and_HP.spam.txt'),\n",
       " (27698,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\5153.2005-07-22.SA_and_HP.spam.txt'),\n",
       " (10451,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\3561.2005-07-02.SA_and_HP.spam.txt'),\n",
       " (21279, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\4311.2005-02-16.GP.spam.txt'),\n",
       " (27052,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\4241.2005-07-17.SA_and_HP.spam.txt'),\n",
       " (24188,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\0205.2002-05-17.SA_and_HP.spam.txt'),\n",
       " (21339, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\4393.2005-02-21.GP.spam.txt'),\n",
       " (4761, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\3862.2005-02-18.GP.spam.txt'),\n",
       " (26798,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3876.2005-07-13.SA_and_HP.spam.txt'),\n",
       " (24854,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1140.2002-09-03.SA_and_HP.spam.txt'),\n",
       " (24851,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1136.2002-09-02.SA_and_HP.spam.txt'),\n",
       " (5134, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\5058.2005-08-23.GP.spam.txt'),\n",
       " (21120, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\4100.2005-01-31.GP.spam.txt'),\n",
       " (26570,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3550.2005-07-05.SA_and_HP.spam.txt'),\n",
       " (10556,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\4057.2005-07-05.SA_and_HP.spam.txt'),\n",
       " (4211, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\1910.2004-08-20.GP.spam.txt'),\n",
       " (27193,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\4448.2005-07-19.SA_and_HP.spam.txt'),\n",
       " (21950, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\5209.2005-05-16.GP.spam.txt'),\n",
       " (21912, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\5158.2005-05-11.GP.spam.txt'),\n",
       " (27700,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\5155.2005-07-22.SA_and_HP.spam.txt'),\n",
       " (33370, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\5540.2005-07-02.BG.spam.txt'),\n",
       " (18746, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\0932.2004-04-22.GP.spam.txt'),\n",
       " (3758, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\0332.2004-01-29.GP.spam.txt'),\n",
       " (21936, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\5190.2005-05-14.GP.spam.txt'),\n",
       " (26753,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3815.2005-07-12.SA_and_HP.spam.txt'),\n",
       " (4230, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\1986.2004-08-26.GP.spam.txt'),\n",
       " (30060, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\1134.2004-10-19.BG.spam.txt'),\n",
       " (21654, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\4817.2005-04-05.GP.spam.txt'),\n",
       " (25239,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1674.2005-06-22.SA_and_HP.spam.txt'),\n",
       " (32847, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\4831.2005-05-15.BG.spam.txt'),\n",
       " (15231, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\0710.2004-10-02.BG.spam.txt'),\n",
       " (18479, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\0576.2004-03-12.GP.spam.txt'),\n",
       " (9568,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\0152.2002-05-10.SA_and_HP.spam.txt'),\n",
       " (15398, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\1304.2004-11-10.BG.spam.txt'),\n",
       " (20598, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\3413.2004-12-04.GP.spam.txt'),\n",
       " (31180, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\2626.2005-01-10.BG.spam.txt'),\n",
       " (24288,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\0346.2002-05-30.SA_and_HP.spam.txt'),\n",
       " (24548,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\0718.2002-07-23.SA_and_HP.spam.txt'),\n",
       " (30569, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\1815.2004-11-25.BG.spam.txt'),\n",
       " (26998,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\4161.2005-07-16.SA_and_HP.spam.txt'),\n",
       " (15729, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\2508.2005-01-19.BG.spam.txt'),\n",
       " (26078,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\2859.2005-06-30.SA_and_HP.spam.txt'),\n",
       " (5094, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\4924.2005-07-26.GP.spam.txt'),\n",
       " (10893,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\5321.2005-07-19.SA_and_HP.spam.txt'),\n",
       " (25759,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\2408.2005-06-27.SA_and_HP.spam.txt'),\n",
       " (16372, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\4917.2005-06-21.BG.spam.txt'),\n",
       " (20276, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\2987.2004-11-04.GP.spam.txt'),\n",
       " (19798, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\2340.2004-09-13.GP.spam.txt'),\n",
       " (30171, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\1281.2004-10-28.BG.spam.txt'),\n",
       " (20260, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\2964.2004-11-03.GP.spam.txt'),\n",
       " (25682,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\2293.2005-06-26.SA_and_HP.spam.txt'),\n",
       " (10831,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\5098.2005-07-19.SA_and_HP.spam.txt'),\n",
       " (26727,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3780.2005-07-07.SA_and_HP.spam.txt'),\n",
       " (19217, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\1563.2004-06-27.GP.spam.txt'),\n",
       " (18255, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\0283.2004-01-27.GP.spam.txt'),\n",
       " (15104, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\0239.2004-08-26.BG.spam.txt'),\n",
       " (19914, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\2496.2004-09-26.GP.spam.txt'),\n",
       " (24838,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1120.2002-08-30.SA_and_HP.spam.txt'),\n",
       " (20714, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\3564.2004-12-16.GP.spam.txt'),\n",
       " (18109, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\0095.2003-12-31.GP.spam.txt'),\n",
       " (26649,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3668.2005-07-06.SA_and_HP.spam.txt'),\n",
       " (10871,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\5266.2005-07-19.SA_and_HP.spam.txt'),\n",
       " (25359,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1835.2005-06-23.SA_and_HP.spam.txt'),\n",
       " (32285, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\4076.2005-03-24.BG.spam.txt'),\n",
       " (26705,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3747.2005-07-06.SA_and_HP.spam.txt'),\n",
       " (19098, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\1404.2004-06-09.GP.spam.txt'),\n",
       " (30417, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\1611.2004-11-15.BG.spam.txt'),\n",
       " (26244,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3089.2005-07-01.SA_and_HP.spam.txt'),\n",
       " (30569, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\1815.2004-11-25.BG.spam.txt'),\n",
       " (15496, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\1652.2004-12-01.BG.spam.txt'),\n",
       " (3969, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\1044.2004-05-06.GP.spam.txt'),\n",
       " (25556,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\2111.2005-06-25.SA_and_HP.spam.txt'),\n",
       " (15572, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\1938.2004-12-19.BG.spam.txt'),\n",
       " (31929, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\3618.2005-02-23.BG.spam.txt'),\n",
       " (4813, 'spampy/datasets/enron\\\\enron1\\\\spam\\\\4004.2005-03-08.GP.spam.txt'),\n",
       " (25317,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1777.2005-06-22.SA_and_HP.spam.txt'),\n",
       " (26320,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3195.2005-07-02.SA_and_HP.spam.txt'),\n",
       " (32108, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\3847.2005-03-10.BG.spam.txt'),\n",
       " (15783, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\2701.2005-01-26.BG.spam.txt'),\n",
       " (15049, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\0034.2004-08-05.BG.spam.txt'),\n",
       " (10616,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\4291.2005-07-06.SA_and_HP.spam.txt'),\n",
       " (19297, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\1668.2004-07-08.GP.spam.txt'),\n",
       " (22082, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\5389.2005-06-09.GP.spam.txt'),\n",
       " (18746, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\0932.2004-04-22.GP.spam.txt'),\n",
       " (15638, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\2199.2005-01-01.BG.spam.txt'),\n",
       " (27178,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\4427.2005-07-18.SA_and_HP.spam.txt'),\n",
       " (25081,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1452.2005-06-14.SA_and_HP.spam.txt'),\n",
       " (20535, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\3330.2004-11-30.GP.spam.txt'),\n",
       " (22229, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\5584.2005-07-05.GP.spam.txt'),\n",
       " (29581, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\0489.2004-09-06.BG.spam.txt'),\n",
       " (16154, 'spampy/datasets/enron\\\\enron3\\\\spam\\\\4096.2005-04-23.BG.spam.txt'),\n",
       " (32122, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\3863.2005-03-11.BG.spam.txt'),\n",
       " (11019,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\5828.2005-07-22.SA_and_HP.spam.txt'),\n",
       " (24548,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\0718.2002-07-23.SA_and_HP.spam.txt'),\n",
       " (30107, 'spampy/datasets/enron\\\\enron6\\\\spam\\\\1197.2004-10-23.BG.spam.txt'),\n",
       " (26713,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\3758.2005-07-07.SA_and_HP.spam.txt'),\n",
       " (20412, 'spampy/datasets/enron\\\\enron4\\\\spam\\\\3165.2004-11-17.GP.spam.txt'),\n",
       " (25164,\n",
       "  'spampy/datasets/enron\\\\enron5\\\\spam\\\\1568.2005-06-21.SA_and_HP.spam.txt'),\n",
       " (10169,\n",
       "  'spampy/datasets/enron\\\\enron2\\\\spam\\\\2495.2005-06-25.SA_and_HP.spam.txt')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from secml.array import CArray\n",
    "from secml.adv.attacks.evasion import CAttackEvasionPGD\n",
    "\n",
    "nb_attack=100\n",
    "\n",
    "class_to_attack=1\n",
    "cnt = 0\n",
    "\n",
    "ori_examples2_x = []\n",
    "ori_examples2_y = []\n",
    "number_list = []\n",
    "for i in range(nb_attack):\n",
    "    #take a point at random being the starting point of the attack\n",
    "    idx_candidates = np.where(y_test == class_to_attack)\n",
    "    #select nb_init_pts points randomly in candidates and make them move\n",
    "    rn = np.random.choice(idx_candidates[0].size, 1)\n",
    "    x0,y0 =ts_set[idx_candidates[0][rn[0]],:].X, ts_set[idx_candidates[0][rn[0]],:].Y\n",
    "    number_list.append(x_test_path[idx_candidates[0][rn[0]]])\n",
    "    \n",
    "    x0=x0.astype(float)\n",
    "    y0=y0.astype(int)\n",
    "    x2 = x0.tondarray()[0]\n",
    "    y2 = y0.tondarray()[0]\n",
    "    \n",
    "    ori_examples2_x.append(x2)\n",
    "    ori_examples2_y.append(y2)\n",
    "    \n",
    "number_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Number: 0\n",
      "Current Number: 1\n",
      "Current Number: 2\n",
      "Current Number: 3\n",
      "Current Number: 4\n",
      "Current Number: 5\n",
      "Current Number: 6\n",
      "Current Number: 7\n",
      "Current Number: 8\n",
      "Current Number: 9\n",
      "Current Number: 10\n",
      "Current Number: 11\n",
      "Current Number: 12\n",
      "Current Number: 13\n",
      "Current Number: 14\n",
      "Current Number: 15\n",
      "Current Number: 16\n",
      "Current Number: 17\n",
      "Current Number: 18\n",
      "Current Number: 19\n",
      "Current Number: 20\n",
      "Current Number: 21\n",
      "Current Number: 22\n",
      "Current Number: 23\n",
      "Current Number: 24\n",
      "Current Number: 25\n",
      "Current Number: 26\n",
      "Current Number: 27\n",
      "Current Number: 28\n",
      "Current Number: 29\n",
      "Current Number: 30\n",
      "Current Number: 31\n",
      "Current Number: 32\n",
      "Current Number: 33\n",
      "Current Number: 34\n",
      "Current Number: 35\n",
      "Current Number: 36\n",
      "Current Number: 37\n",
      "Current Number: 38\n",
      "Current Number: 39\n",
      "Current Number: 40\n",
      "Current Number: 41\n",
      "Current Number: 42\n",
      "Current Number: 43\n",
      "Current Number: 44\n",
      "Current Number: 45\n",
      "Current Number: 46\n",
      "Current Number: 47\n",
      "Current Number: 48\n",
      "Current Number: 49\n",
      "Current Number: 50\n",
      "Current Number: 51\n",
      "Current Number: 52\n",
      "Current Number: 53\n",
      "Current Number: 54\n",
      "Current Number: 55\n",
      "Current Number: 56\n",
      "Current Number: 57\n",
      "Current Number: 58\n",
      "Current Number: 59\n",
      "Current Number: 60\n",
      "Current Number: 61\n",
      "Current Number: 62\n",
      "Current Number: 63\n",
      "Current Number: 64\n",
      "Current Number: 65\n",
      "Current Number: 66\n",
      "Current Number: 67\n",
      "Current Number: 68\n",
      "Current Number: 69\n",
      "Current Number: 70\n",
      "Current Number: 71\n",
      "Current Number: 72\n",
      "Current Number: 73\n",
      "Current Number: 74\n",
      "Current Number: 75\n",
      "Current Number: 76\n",
      "Current Number: 77\n",
      "Current Number: 78\n",
      "Current Number: 79\n",
      "Current Number: 80\n",
      "Current Number: 81\n",
      "Current Number: 82\n",
      "Current Number: 83\n",
      "Current Number: 84\n",
      "Current Number: 85\n",
      "Current Number: 86\n",
      "Current Number: 87\n",
      "Current Number: 88\n",
      "Current Number: 89\n",
      "Current Number: 90\n",
      "Current Number: 91\n",
      "Current Number: 92\n",
      "Current Number: 93\n",
      "Current Number: 94\n",
      "Current Number: 95\n",
      "Current Number: 96\n",
      "Current Number: 97\n",
      "Current Number: 98\n",
      "Current Number: 99\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Perform adversarial attacks\n",
    "noise_type = 'l2'  # Type of perturbation 'l1' or 'l2'\n",
    "dmax = 0.09 # Maximum perturbation\n",
    "lb, ub = 0, 1  # Bounds of the attack space. Can be set to `None` for unbounded\n",
    "\n",
    "solver_params = {\n",
    "    'eta': 0.01,\n",
    "    'max_iter': 20,\n",
    "    'eps': 1e-6}\n",
    "\n",
    "#set lower bound and upper bound respectively to 0 and 1 since all features are Boolean\n",
    "pgd_attack = CAttackEvasionPGD(\n",
    "    classifier=clf_lin,\n",
    "    double_init_ds=tr_set,\n",
    "    distance=noise_type,\n",
    "    dmax=dmax,\n",
    "    lb=lb, ub=ub,\n",
    "    solver_params=solver_params)\n",
    "\n",
    "\n",
    "ad_examples_x = []\n",
    "ad_examples_y = []\n",
    "cnt = 0\n",
    "for i in range(len(ori_examples2_x)):\n",
    "    print(\"Current Number:\", i)\n",
    "    x0 = ori_examples2_x[i]\n",
    "    y0 = ori_examples2_y[i]\n",
    "\n",
    "    y_pred_pgd, _, adv_ds_pgd, _ = pgd_attack.run(x0, y0)\n",
    "\n",
    "    if y_pred_pgd.item() == 0:\n",
    "        cnt = cnt + 1\n",
    "\n",
    "    ad_examples_x.append(adv_ds_pgd.X.tondarray()[0])\n",
    "    ad_examples_y.append(y_pred_pgd.item())\n",
    "\n",
    "    attack_pt = adv_ds_pgd.X.tondarray()[0]\n",
    "print(\"Accuracy:\", cnt/nb_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>5.987693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>7.801431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaa</th>\n",
       "      <td>10.104016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaaci</th>\n",
       "      <td>10.509482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaaahhhhhh</th>\n",
       "      <td>10.509482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzyudgpd</th>\n",
       "      <td>10.509482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzglvaa</th>\n",
       "      <td>10.509482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzxlqbha</th>\n",
       "      <td>10.509482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzz</th>\n",
       "      <td>8.430040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzzzcard</th>\n",
       "      <td>10.509482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107866 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "aa           5.987693\n",
       "aaa          7.801431\n",
       "aaaa        10.104016\n",
       "aaaaci      10.509482\n",
       "aaaahhhhhh  10.509482\n",
       "...               ...\n",
       "zzyudgpd    10.509482\n",
       "zzzglvaa    10.509482\n",
       "zzzxlqbha   10.509482\n",
       "zzzz         8.430040\n",
       "zzzzcard    10.509482\n",
       "\n",
       "[107866 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_examples2_x = np.array(ori_examples2_x)\n",
    "ori_examples2_y = np.array(ori_examples2_y)\n",
    "ad_examples_x = np.array(ad_examples_x)\n",
    "ad_examples_y = np.array(ad_examples_y)\n",
    "\n",
    "ori_dataframe = pd.DataFrame(ori_examples2_x, columns = vect.get_feature_names())\n",
    "ad_dataframe = pd.DataFrame(ad_examples_x, columns = vect.get_feature_names())\n",
    "\n",
    "ad_dataframe['ad_label'] = ad_examples_y\n",
    "ad_success = ad_dataframe.loc[ad_dataframe.ad_label == 0]\n",
    "ori_success = ori_dataframe.loc[ad_dataframe.ad_label == 0]\n",
    "ad_fail = ad_dataframe.loc[ad_dataframe.ad_label == 1]\n",
    "ori_fail = ori_dataframe.loc[ad_dataframe.ad_label == 1]\n",
    "\n",
    "ad_success_x = ad_success.drop(columns = ['ad_label'])\n",
    "ad_fail_x = ad_fail.drop(columns = ['ad_label'])\n",
    "result = (ad_success_x - ori_success)\n",
    "\n",
    "vect.idf_\n",
    "IDF = pd.DataFrame(vect.idf_.T, index=vect.get_feature_names())\n",
    "IDF.to_csv(\"idf.csv\")\n",
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>louis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sitara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>yahoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>mw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>clickathom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>tom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0        enron\n",
       "1        louis\n",
       "2         vinc\n",
       "3       attach\n",
       "4        thank\n",
       "..         ...\n",
       "95      sitara\n",
       "96       yahoo\n",
       "97          mw\n",
       "98  clickathom\n",
       "99         tom\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "x2result1 = result\n",
    "x2result1 = np.array(x2result1)\n",
    "x2result = result\n",
    "x2result = x2result.multiply(x2result1)\n",
    "\n",
    " \n",
    "sum_number = x2result.sum()/cnt\n",
    "\n",
    "\n",
    "sum_number = pd.DataFrame(sum_number, columns = ['sum_number'])\n",
    "sum_number = sum_number.sort_values(by='sum_number', ascending=False, inplace=False)\n",
    "sum_number\n",
    "#sum_number = sum_number.loc[var_number['var_number']>0]\n",
    "#print(sum_number.index[:200])\n",
    "#print(sum_number.index[:500])\n",
    "\n",
    "sum_number_pd = pd.DataFrame(sum_number.index[:100])\n",
    "sum_number_pd.to_csv(\"x2result.csv\")\n",
    "sum_number_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori > 0, ad = 0\n",
    "# The dispearing features\n",
    "ad1 = ad_success_x\n",
    "ori1 = ori_success\n",
    "\n",
    "ori2 = ori1.loc[:, (ori1>=0).all(axis=0)]\n",
    "ori = ori2.loc[:, ~(ori2==0).all(axis=0)]\n",
    "\n",
    "ad = ad1.loc[:,ori.columns]\n",
    "ad = ad.loc[:, (ad>=0).all(axis=0)]\n",
    "ad = ad.loc[:, ~(ad>0).all(axis=0)]\n",
    "\n",
    "ori1_ad0_columns = ad.columns\n",
    "ori1_ad0_columns = pd.DataFrame(ori1_ad0_columns)\n",
    "ori1_ad0_columns.to_csv(\"ori1_ad0_columns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori = 0, ad > 0\n",
    "# The adding features\n",
    "ad11 = ad_success_x\n",
    "ori11 = ori_success\n",
    "\n",
    "ori21 = ori11.loc[:, (ori11>=0).all(axis=0)]\n",
    "ori22 = ori21.loc[:, ~(ori21>0).all(axis=0)]\n",
    "\n",
    "ad22 = ad11.loc[:,ori22.columns]\n",
    "ad22 = ad22.loc[:, (ad22>=0).all(axis=0)]\n",
    "ad22 = ad22.loc[:, ~(ad22==0).all(axis=0)]\n",
    "\n",
    "ori0_ad1_columns = ad22.columns\n",
    "ori0_ad1_columns = pd.DataFrame(ori0_ad1_columns)\n",
    "ori0_ad1_columns.to_csv(\"ori0_ad1_columns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1096\n",
      "2 2185\n",
      "3 3246\n",
      "4 6419\n",
      "5 9079\n",
      "6 12221\n",
      "Count:  12221\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "cut_model = nltk.WordPunctTokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# words14 = \"enrononlin fundi cera dailyupd jobsearch congrad ena listbot counterparti calger sitara jhherbert kaminski solarc clickathom\"\n",
    "# words14 = \"eyeforenergi ena counterparti kaminski enrononlin clickathom jhherbert cera topica calger sitara cdnow beenladen\"\n",
    "words14 = \" enrononlin ena reactionsnet cdnow kaminski lokay sitara counterparti clickathom topica cera eyeforenergi\"\n",
    "spam_cnt = 0\n",
    "d2 = \"spampy/datasets/enron/\"\n",
    "for r in range(1, 7):\n",
    "    d3 = d2 + \"enron\" + str(r)+\"/spam\"\n",
    "    emails2 = [os.path.join(d3, f) for f in os.listdir(d3)]\n",
    "    for j in emails2:\n",
    "        with codecs.open(j, \"rb\", encoding='utf_8_sig', errors='ignore') as m:\n",
    "            #print(j)\n",
    "            choose_email = []\n",
    "            line_str = \"\"\n",
    "            for line in m:\n",
    "                for word in line:\n",
    "                    if word.startswith(\"http\"):\n",
    "                        word = \"URL\"\n",
    "                    word = stemmer.stem(word)\n",
    "\n",
    "                line = re.sub(r'[^a-zA-Z\\s]', '', string=line)\n",
    "                line = line.lower()\n",
    "                line = line.strip()\n",
    "                tokens = cut_model.tokenize(line)\n",
    "                line = [stemmer.stem(token) for token in tokens if token not in stopwords]\n",
    "\n",
    "                line = ' '.join(line)\n",
    "                line_str = line_str+line+\" \"\n",
    "            line_str = line_str+words14\n",
    "            choose_email.append(line_str)\n",
    "        message_14_email = pd.DataFrame(choose_email, columns = [\"message\"])\n",
    "        message_14_tf_idf = vect.transform(message_14_email[\"message\"])\n",
    "        message_14_tf_idf = pd.DataFrame(message_14_tf_idf.toarray(), columns = vect.get_feature_names())\n",
    "        #print(message_14_tf_idf)\n",
    "        message_14_y = [1]\n",
    "        message_14_y = pd.Series(message_14_y)\n",
    "        message_CData = CDataset(message_14_tf_idf, message_14_y)\n",
    "        message_14_pred = clf_lin.predict(message_CData.X)\n",
    "        # print(message_14_pred)\n",
    "        if message_14_pred == 0:\n",
    "            spam_cnt = spam_cnt+1\n",
    "        #break\n",
    "    print(r, spam_cnt)\n",
    "print(\"Count: \", spam_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
